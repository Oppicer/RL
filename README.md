# Qwenâ€¯3â€¯1.7â€¯B â€” *RTXâ€¯3070â€‘Optimised* Codeâ€‘Creation Agent

**With Alphaâ€‘Evolveâ€‘Style Selfâ€‘Improvement & Strict 8â€¯GB VRAM Budget**

> **Mission**â€‚Ship a *stateâ€‘ofâ€‘theâ€‘art* subâ€‘2â€¯B coding model that (a) fits a **single RTXâ€¯3070Â (8â€¯GB)**, (b) trains in **â‰¤â€¯1Â week wallâ€‘clock**, (c) uses the **latest lowâ€‘compute tricks**, **GRM** *and* a lightweight **Alphaâ€‘Evolve loop** for continual improvement, (d) exposes safe webâ€‘search toolâ€‘calls and (e) runs behind a friendly Gradio GUI.

---

## 0Â Â·Â Hard Restrictions

| Limitation                         | Design Response                                                                      |
| ---------------------------------- | ------------------------------------------------------------------------------------ |
| **VRAMÂ â‰¤â€¯8â€¯GB**                    | 4â€‘bit NF4 QLoRA + gradientâ€‘checkpointing + paged optimisers; FP16/FP32 only on CPU   |
| **SystemÂ RAMÂ =â€¯32â€¯GB**             | Offâ€‘load optimiser states & KVâ€‘cache to CPU via `accelerate`; keep dataset streaming |
| **No paid APIs during training**   | All webâ€‘lookups cached; SerpAPI only in inference time, never during SFT/GRM         |
| **Wallâ€‘clock â‰¤â€¯7Â days**            | Progressive Data Dropout + Alphaâ€‘Evolve population sizeÂ 4 + earlyâ€‘stop heuristics    |
| **Openâ€‘source licence compliance** | Use only OSIâ€‘approved code (TheÂ StackÂ v2 filtered) and MITâ€‘licensed training scripts |

---

## 1Â Â·Â Why this build?

| ðŸš€ Innovation            | What we borrow from cuttingâ€‘edge research                                                                                      |
| ------------------------ | ------------------------------------------------------------------------------------------------------------------------------ |
| **FP4â€‘ready kernels**    | FlashAttentionâ€‘3 & TritonÂ 2.1 fused MQA (from NVIDIA Blackwell papers)                                                         |
| **QLoRAâ€‘NF4 + LoRAâ€‘RSD** | 4â€‘bit base + *Rankâ€‘Switchable* Dropâ€‘in adapters (HiLo 2025) recover dense accuracy at <0.5â€¯% params                            |
| **GRMâ€‘lite**             | DeepSeekÂ R2 generative reward modelling without PPOÂ â†’ zero human labels                                                        |
| **Alphaâ€‘Evolve loop**    | Inspired by DeepMind *AlphaDev/AlphaEvolve*: populationâ€‘based evolutionary search over hyperâ€‘params & codeâ€‘challenge curricula |
| **Specâ€‘decoding**        | vLLM draftâ€‘&â€‘target (int8Â +Â 4â€‘bit) gives 12â€‘15 tok/s generation on 3070                                                        |

---

## 2Â Â·Â QuickÂ Start (TL;DR)

```bash
# 0Â â€“Â Env & libs (Ampereâ€‘friendly)
conda create -n q3070 python=3.10 -y && conda activate q3070
pip install "unsloth[flash-attn]" transformers accelerate bitsandbytes peft \
            datasets tiktoken wandb triton==2.1.0 ray==2.11.0 vllm==0.4.0

# 1Â â€“Â Pull 4â€‘bit baseweights to GPU, rest to CPU
python scripts/pull_4bit.py  # wrapped snippet from sectionÂ 3

# 2Â â€“Â Run the full pipeline (SFTÂ â†’Â GRMÂ â†’Â Evolve)
python pipelines/train_full.py --config configs/rtx3070.yaml
```

*Details of each phase below.*

---

## 3Â Â·Â Dataset & Preâ€‘processing (50â€¯K files)

```bash
bash data/build_dataset.sh   # TheÂ StackÂ v2 + LeetCode + HumanEvalâ€‘Aug
```

* Cleans, dedups, tokenises to **2â€¯048â€¯tok** chunks.
* Streams shards to keep RAM â‰¤â€¯6â€¯GB.

---

## 4Â Â·Â PhaseÂ 1 â€” Supervised Fineâ€‘Tune (SFT)

* 4â€¯epochs on 320â€¯M tokens.
* **QLoRA hyperâ€‘params** â†’ `lora_rÂ 16`, `alphaÂ 32`, `dropoutÂ 0.05`, `rank_dropoutÂ 0.08` (*Rankâ€‘Switchable*).
* **PDD** cuts tokens 50â€¯% after epochÂ 1.
* **ETA:** 65â€‘80â€¯h @Â \~45â€¯tok/s.

---

## 5Â Â·Â PhaseÂ 2 â€” GRMâ€‘lite (Selfâ€‘Reward)

* 2â€¯000 prompts Ã— 10 drafts â†’ best 2 scored by 220â€¯M critic.
* **UPFT** prefixâ€‘tune (+3â€¯pp HumanEval).
* **ETA:** 12â€¯h CPUâ€‘only (runs while you sleep).

---

## 6Â Â·Â PhaseÂ 3 â€” *Alphaâ€‘Evolve* Mini Popâ€‘Based Training

Borrowing DeepMindâ€™s *AlphaEvolve* idea but downsized for a single PC:

1. **PopulationÂ 4**: clone base + 3 mutants (different `lora_r`, LR, seq\_len).
2. **Curriculum**: start with *easy* HumanEval, escalate to APPSâ€‘Dev hard subset.
3. **Fitness** = pass\@1â€¯Ã—â€¯speed bonusâ€¯Ã—â€¯(1Â â€“Â KLÂ divergence from base).
4. **Evolution loop** (Ray actors):

   * run 1Â epoch microâ€‘SFT per individual (â‰ˆÂ 4â€¯h)
   * select topÂ 2 â†’ mutate hyperâ€‘params
   * iterate 3Â generations (\~12â€¯h total, GPU load serialised).
5. **Merge**: keep best LoRA adapter (`loras/best_alpha_evolve.safetensors`).

> **Outcome:** additional **+4â€‘6â€¯pp** HumanEval and noticeably better *multiâ€‘file repo coherence*.

---

## 7Â Â·Â Agentic Toolâ€‘Calls for DocsÂ Lookup (Safety First)

| Policy guard            | Implementation                                                                 |
| ----------------------- | ------------------------------------------------------------------------------ |
| **Domain allowâ€‘list**   | Only docs.python.org, fastapi.tiangolo.com, readthedocs.io, stackoverflow\.com |
| **Snippet length cap**  | 500â€¯words per call; truncation at sentence boundary                            |
| **Rate limit**          | 5Â searches / user request                                                      |
| **No trainâ€‘time calls** | `ENV=offline` flag blocks `web_search` during training stages                  |

JSON schema identical to sectionÂ 6 earlier.

---

## 8Â Â·Â Evaluation Benchmarks

| Dataset              | Target after Alphaâ€‘Evolve |
| -------------------- | ------------------------- |
| **HumanEval**        | **56â€‘58â€¯%** pass\@1       |
| **MBPPâ€‘Aug**         | 60â€¯%                      |
| **APPSâ€‘Dev Medium**  | 37â€¯%                      |
| **ToolBenchâ€‘Coding** | 68â€¯% success              |

---

## 9Â Â·Â Serving & GUI

* **vLLM API**: 12â€‘15 tok/s (`gpu_memory_utilizationÂ 0.90`, draft int8).
* **Gradio IDE** (`gui/gradio_app.py`) autoâ€‘detects toolâ€‘calls and shows them in a sidebar.

---

## 10Â Â·Â Training Timeline (Wallâ€‘clock)

| Phase            | Duration | Cumulative         |
| ---------------- | -------- | ------------------ |
| SFT              | 3â€‘4Â days | 4Â d                |
| GRMâ€‘lite         | +12â€¯h    | 4.5Â d              |
| Alphaâ€‘Evolve     | +1â€¯d     | 5.5Â d              |
| Eval + GUI build | +4â€¯h     | **â‰ˆâ€¯6Â days total** |

---

## 11Â Â·Â Troubleshooting

* **OOM** â†’ reduce `seq_len` to 1â€¯024 or `micro_batch` toÂ 2.
* **Slow SFT** â†’ disable rankâ€‘switchable adapters (`rank_dropoutÂ 0`).
* **Evolve loop stalls** â†’ lower population toÂ 2 or increase Ray object store size (`RAY_memory=8G`).

---

## 12Â Â·Â Roadmap

* [ ] Plugâ€‘in *runâ€‘codeâ€‘inâ€‘sandbox* tool for executionâ€‘based rewards.
* [ ] Add FP4 weight dump when NVIDIA releases Ampere FP4 kernels.
* [ ] Docker Compose miniâ€‘stack (APIÂ +Â GUIÂ +Â Redis cache).

---

## 13Â Â·Â References

* QLoRAâ€‘NF4: *DettmersÂ etÂ al., 2024*
* Rankâ€‘Switchable Dropout (RSD): *HiLoÂ Adapters, 2025*
* Generative Reward Modelling: *DeepSeekÂ R2 Tech Report, 2025*
* Alphaâ€‘Evolve: *Google DeepMind Blog, JanÂ 2025*
* FlashAttentionâ€‘3: *DaoÂ etÂ al., 2025*
* PDD: *RaffelÂ etÂ al., 2025*
